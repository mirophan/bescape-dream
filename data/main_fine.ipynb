{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dream Challenge deconvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, ShuffleSplit,GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import NuSVR, LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DREAM input and our GEPs\n",
    "Need to specify:\n",
    "* in_dir\n",
    "* gold_standards_file\n",
    "* gold_standards_dir\n",
    "* signature_path\n",
    "* drop['others']?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker = False\n",
    "coarse = True\n",
    "\n",
    "if docker:\n",
    "    in_name = \"input.csv\"\n",
    "    in_dir = \"./input\"\n",
    "    in_path = os.path.join(in_dir, in_name)\n",
    "\n",
    "    out_name= \"predictions.csv\"\n",
    "    out_dir = \"./output\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "\n",
    "    input_df = pd.read_csv(in_path)\n",
    "    crc_gep = pd.read_csv(\"signatures/ileal_merged.csv\", index_col='NAME')\n",
    "    brca_gep = pd.read_csv(\"signatures/brca.gct\", sep='\\t', index_col='NAME')\n",
    "\n",
    "else:\n",
    "    in_name = \"input.csv\"\n",
    "    #in_dir = \"./input/leaderboard_1\"\n",
    "    #in_dir = \"./input/leaderboard_3\"\n",
    "    in_dir = \"./input/roche\"\n",
    "    in_path = os.path.join(in_dir, in_name)\n",
    "    \n",
    "    out_dir = \"./output\"\n",
    "    out_load = \"load.csv\"\n",
    "    if os.path.isfile(os.path.join(out_dir, out_load)):\n",
    "        load_df = pd.read_csv(os.path.join(out_dir, out_load))\n",
    "    else:\n",
    "        load_df = pd.DataFrame( columns = ['algorithm','truth'])\n",
    "        # TODO implement dataset folder naming - dream vs epic, etc\n",
    "        #load_df = pd.DataFrame( columns = ['algorithm', 'validation_file'])\n",
    "        \n",
    "    #####\n",
    "    #gold_standards_file = \"lb_fine_r1\"\n",
    "    #gold_standards_dir = \"./gold_standards/\"\n",
    "    \n",
    "    #gold_standards_file = \"lb_fine_r3\"\n",
    "    #gold_standards_dir = \"./gold_standards/\"\n",
    "    \n",
    "    gold_standards_file = \"GSE134809-truth\"\n",
    "    gold_standards_dir = \"./gold_standards/roche\"\n",
    "    \n",
    "    gold_standards_path = os.path.join(gold_standards_dir, gold_standards_file + '.csv')\n",
    "    \n",
    "    #####\n",
    "    # TODO\n",
    "    #crc_path = \"signatures/GSE134809_human_ileal_Crohns_filtered.gct\"\n",
    "    #crc_path = \"signatures/ileal_merged.csv\"\n",
    "    #crc_path = 'signatures/generated/ileal-smilie-raw-scanorama.csv'\n",
    "    #crc_path = 'signatures/generated/ileal-smilie-raw-union-scanorama.csv'\n",
    "    #crc_path = 'signatures/generated/ileal-smilie-ibd-raw-scanorama.csv'\n",
    "    crc_path = 'signatures/generated/ileal-smilie-brca-neutro-rmCells-scanorama.csv'\n",
    "    #crc_path = 'signatures/generated/ileal-smilie-brca-neutro-rmCells-union-scanorama.csv'\n",
    "    brca_path = \"signatures/brca.gct\"\n",
    "    #signature_path = \"signatures/smillie2019_human_ibd.gct\"\n",
    "    #signature_path = \"signatures/generated/lm22_dream.gct\"\n",
    "    \n",
    "    crc_gep = pd.read_csv(crc_path, index_col='NAME')\n",
    "    brca_gep = pd.read_csv(brca_path, sep='\\t', index_col='NAME')\n",
    "    \n",
    "    input_df = pd.read_csv(in_path)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crc_gep = crc_gep.fillna(0)\n",
    "crc_gep = crc_gep.where(crc_gep != 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signature modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>NK.cells</th>\n",
       "      <th>endothelial.cells</th>\n",
       "      <th>fibroblasts</th>\n",
       "      <th>macrophages</th>\n",
       "      <th>memory.B.cells</th>\n",
       "      <th>memory.CD4.T.cells</th>\n",
       "      <th>memory.CD8.T.cells</th>\n",
       "      <th>monocytes</th>\n",
       "      <th>myeloid.dendritic.cells</th>\n",
       "      <th>naive.B.cells</th>\n",
       "      <th>naive.CD4.T.cells</th>\n",
       "      <th>naive.CD8.T.cells</th>\n",
       "      <th>neutrophils</th>\n",
       "      <th>regulatory.T.cells</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000175899</th>\n",
       "      <td>A2M</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000245105</th>\n",
       "      <td>A2M-AS1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000141338</th>\n",
       "      <td>ABCA8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000231621</th>\n",
       "      <td>AC013264.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000224137</th>\n",
       "      <td>AC079767.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000101443</th>\n",
       "      <td>WFDC2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000143184</th>\n",
       "      <td>XCL1</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000143185</th>\n",
       "      <td>XCL2</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000178381</th>\n",
       "      <td>ZFAND2A</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000176083</th>\n",
       "      <td>ZNF683</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Description  NK.cells  endothelial.cells  fibroblasts  \\\n",
       "NAME                                                                    \n",
       "ENSG00000175899         A2M     0.000              0.067        0.046   \n",
       "ENSG00000245105     A2M-AS1     0.001             -0.001       -0.001   \n",
       "ENSG00000141338       ABCA8     0.000              0.000        0.020   \n",
       "ENSG00000231621  AC013264.2     0.001             -0.001       -0.001   \n",
       "ENSG00000224137  AC079767.4     0.000              0.000        0.000   \n",
       "...                     ...       ...                ...          ...   \n",
       "ENSG00000101443       WFDC2     0.000              0.000        0.003   \n",
       "ENSG00000143184        XCL1     0.036              0.001        0.001   \n",
       "ENSG00000143185        XCL2     0.042             -0.001       -0.001   \n",
       "ENSG00000178381     ZFAND2A     0.003              0.003        0.001   \n",
       "ENSG00000176083      ZNF683     0.002              0.000        0.000   \n",
       "\n",
       "                 macrophages  memory.B.cells  memory.CD4.T.cells  \\\n",
       "NAME                                                               \n",
       "ENSG00000175899        0.012           0.001               0.002   \n",
       "ENSG00000245105        0.000           0.000               0.002   \n",
       "ENSG00000141338        0.000           0.000               0.000   \n",
       "ENSG00000231621        0.000           0.000               0.013   \n",
       "ENSG00000224137        0.001           0.027               0.000   \n",
       "...                      ...             ...                 ...   \n",
       "ENSG00000101443        0.000           0.001               0.001   \n",
       "ENSG00000143184        0.000           0.000               0.002   \n",
       "ENSG00000143185        0.000           0.000               0.001   \n",
       "ENSG00000178381        0.002           0.005               0.007   \n",
       "ENSG00000176083        0.000           0.000               0.002   \n",
       "\n",
       "                 memory.CD8.T.cells  monocytes  myeloid.dendritic.cells  \\\n",
       "NAME                                                                      \n",
       "ENSG00000175899               0.001      0.005                    0.000   \n",
       "ENSG00000245105               0.002      0.000                    0.000   \n",
       "ENSG00000141338               0.000      0.000                    0.000   \n",
       "ENSG00000231621               0.004      0.001                    0.000   \n",
       "ENSG00000224137               0.000      0.001                    0.001   \n",
       "...                             ...        ...                      ...   \n",
       "ENSG00000101443               0.001      0.001                    0.000   \n",
       "ENSG00000143184               0.024      0.000                    0.000   \n",
       "ENSG00000143185               0.020      0.000                    0.000   \n",
       "ENSG00000178381               0.005      0.004                    0.002   \n",
       "ENSG00000176083               0.008      0.000                    0.000   \n",
       "\n",
       "                 naive.B.cells  naive.CD4.T.cells  naive.CD8.T.cells  \\\n",
       "NAME                                                                   \n",
       "ENSG00000175899          0.001              0.001              0.001   \n",
       "ENSG00000245105          0.000              0.001              0.000   \n",
       "ENSG00000141338          0.000              0.000              0.000   \n",
       "ENSG00000231621          0.001              0.020              0.021   \n",
       "ENSG00000224137          0.016              0.000              0.000   \n",
       "...                        ...                ...                ...   \n",
       "ENSG00000101443          0.001              0.001              0.000   \n",
       "ENSG00000143184          0.000              0.002              0.001   \n",
       "ENSG00000143185          0.000              0.000              0.000   \n",
       "ENSG00000178381          0.004              0.010              0.008   \n",
       "ENSG00000176083          0.000              0.001              0.001   \n",
       "\n",
       "                 neutrophils  regulatory.T.cells  \n",
       "NAME                                              \n",
       "ENSG00000175899        0.000               0.001  \n",
       "ENSG00000245105        0.001               0.000  \n",
       "ENSG00000141338        0.000               0.000  \n",
       "ENSG00000231621        0.010               0.003  \n",
       "ENSG00000224137        0.000               0.000  \n",
       "...                      ...                 ...  \n",
       "ENSG00000101443        0.000               0.001  \n",
       "ENSG00000143184        0.017               0.001  \n",
       "ENSG00000143185        0.012              -0.001  \n",
       "ENSG00000178381        0.006               0.008  \n",
       "ENSG00000176083        0.011               0.000  \n",
       "\n",
       "[528 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crc_gep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "crc_gep.drop(['Description'], axis=1, inplace=True)\n",
    "crc_gep = crc_gep.fillna(0)\n",
    "crc_gep = crc_gep.where(crc_gep != 0, 0)\n",
    "#crc_gep.drop(['Description', 'others', 'myeolid.cells'], axis=1, inplace=True)\n",
    "#crc_gep = np.log1p(crc_gep)\n",
    "#crc_gep /= 100 #cp10k -> tpm\n",
    "#crc_gep.drop(['Description'], axis=1, inplace=True)\n",
    "\n",
    "brca_gep.drop(['Description', 'others', 'cancer.cells'], axis=1, inplace=True)\n",
    "brca_gep = brca_gep.fillna(0)\n",
    "brca_gep = brca_gep.where(crc_gep != 0, 0)\n",
    "#brca_gep = np.log1p(brca_gep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528, 14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crc_gep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = input_df['dataset.name']\n",
    "scales = input_df['scale']\n",
    "c_types = input_df['cancer.type']\n",
    "#native_probe = input_df['native.probe.type']\n",
    "#expression_files = input_df['hugo.expr.file']\n",
    "expression_files = input_df['ensg.expr.file']\n",
    "expression_paths = []\n",
    "\n",
    "for file in expression_files:\n",
    "    expression_paths.append(os.path.join(in_dir, file))\n",
    "    \n",
    "\n",
    "def load_expression_file(expression_path):\n",
    "    expression_df = pd.read_csv(expression_path, index_col = 'Gene')\n",
    "    return expression_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# deal with duplicate genes in sample bulk and in signature genes\n",
    "# Note: do not merge samples - not all genes are intersecting across samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nu-SVR without CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersect_genes(bulk_sample, signature_matrix):\n",
    "    \"\"\"Find intersecting subset of genes between sample and signature matrix\"\"\"\n",
    "    #TODO add warning if only few genes are common\n",
    "    \n",
    "    # drop rows with nan\n",
    "    bulk_sample_notna = bulk_sample[pd.notna(bulk_sample.iloc[:,0:1]).any(axis=1)]\n",
    "    \n",
    "    idx = bulk_sample_notna.index & signature_matrix.index\n",
    "    return bulk_sample_notna.loc[idx], signature_matrix.loc[idx]\n",
    "\n",
    "\n",
    "def build_model(bulk_sample_subset, signature_matrix_subset):\n",
    "    \"\"\"Create and fit a regression model to bulk RNA (y) and a signature matrix (X)\n",
    "    \n",
    "    y = w * X | w = 'predicted cell type fractions'\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        bulk_sample_subset (pandas.DataFrame): bulk RNA\n",
    "        signatre_matrix_subset (pandas.DataFrame): GEP\n",
    "    \n",
    "    Returns:\n",
    "        Instance of the regression model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scale', StandardScaler()),\n",
    "        ('svr', NuSVR())\n",
    "    ])\n",
    "    \n",
    "    cv = ShuffleSplit(test_size=0.01, n_splits=1) # no CV - we want to minimize training error and not generalization error\n",
    "    parameters = {'svr__nu' : [0.25, 0.5, 0.75], 'svr__C' : [1e-2,1e-1,1], 'svr__kernel' : ['linear'], 'svr__verbose' : [True]}\n",
    "    grid = GridSearchCV(pipe, param_grid = parameters, cv = cv, scoring = 'neg_mean_squared_error', verbose=10, n_jobs=-1)\n",
    "    grid.fit(signature_matrix_subset, bulk_sample_subset.values.ravel())\n",
    "\n",
    "    return grid\n",
    "\n",
    "# following methods are temporary\n",
    "def rm_neg(coefs):\n",
    "    \"\"\"Removes negative coefficients from the weight vector\n",
    "    \n",
    "    Args:\n",
    "        coefs (numpy.array): coefficients from the regression model, representing cell fractions\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array with non-negative values\n",
    "    \n",
    "    \"\"\"\n",
    "    c = coefs.copy()\n",
    "    c[c < 0] = 0 \n",
    "    return c\n",
    "\n",
    "def rm_small(coefs):\n",
    "    \"\"\"Remove small weight vector coefficents based on an arbitrary cutoff\"\"\"\n",
    "    # improvement: use standard deviation\n",
    "    c = coefs.copy()\n",
    "    c[c < 0.00001] = 0\n",
    "    return c\n",
    "\n",
    "def rescale (coefs):\n",
    "    \"\"\"Rescale the weight vector so that the total sums up to 1\"\"\"\n",
    "    c = coefs.copy()\n",
    "    scale_factor = 1.0 / (np.sum(c))\n",
    "    c *= scale_factor\n",
    "    return c\n",
    "\n",
    "def append_missing_celltype(df, dataset_name, sample, cell_type, prediction=0):\n",
    "    \"\"\"Appends missing cell type to the output\n",
    "    \n",
    "    Useful until we are able to integrate signatures with a full set of cell types\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): result df to be appended to\n",
    "        dataset_name (string)\n",
    "        sample (string)\n",
    "        cell_type (string)\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame with appended celltype and its predicted proportion set to 0\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    res_df = df.copy()\n",
    "    append_df = { \"dataset.name\" : dataset_name, \n",
    "                 \"sample.id\" : sample, \n",
    "                 \"cell.type\" : cell_type, \n",
    "                 \"prediction\" : prediction}\n",
    "    res_df = res_df.append(append_df, ignore_index=True)\n",
    "    return res_df\n",
    "\n",
    "def linearize(bulk, scale):\n",
    "    \"\"\"Takes anti-log transformation on log scaled data\n",
    "    \n",
    "    Some data inputs from the DREAM challenge are log2 or log10 scaled and need to be linearized.\n",
    "    \n",
    "    Args:\n",
    "        bulk (pandas.DataFrame): Bulk expression matrix\n",
    "        scale (String): The scale of the expression data (i.e., Log2, Log10, Linear)\n",
    "    Returns:\n",
    "        Linearized bulk expression matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if scale == 'Log2':\n",
    "        return 2 ** bulk\n",
    "    elif scale == 'Log10':\n",
    "        return 10 ** bulk\n",
    "    else:\n",
    "        return bulk\n",
    "    \n",
    "def to_coarse(result_df):\n",
    "    \"\"\"Converts fine grained cell type predictions to coarse grained. \n",
    "    \n",
    "    This is done by simply summing up the fractions.\n",
    "    \n",
    "    Args:\n",
    "        result_df (pandas.DataFrame): Result table with predicted fine grained cell type fractions as defined in the DREAM challenge\n",
    "    Returns:\n",
    "        pandas.DataFrame result table with coarse-grained cell type predictions as\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dict_coarse = {'memory.B.cells' : 'B.cells', \n",
    "               'naive.B.cells' : 'B.cells',\n",
    "               'memory.CD4.T.cells' : 'CD4.T.cells',\n",
    "               'naive.CD4.T.cells' : 'CD4.T.cells',\n",
    "               'regulatory.T.cells' : 'CD4.T.cells',\n",
    "               'memory.CD8.T.cells' : 'CD8.T.cells',\n",
    "               'naive.CD8.T.cells' : 'CD8.T.cells',\n",
    "               'monocytes' : 'monolytic.lineage',\n",
    "               'myeloid.dendritic.cells' : 'monolytic.lineage',\n",
    "               'macrophages' : 'monolytic.lineage',\n",
    "              }\n",
    "\n",
    "    coarse_df = result_df.replace({'cell.type' : dict_coarse})\n",
    "    coarse_df = coarse_df.groupby(['dataset.name', 'sample.id', 'cell.type']).sum().reset_index()\n",
    "    return coarse_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deconvoluting dataset GSE134809 and sample [name: S0 ] [1 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    2.6s remaining:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    2.6s remaining:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    2.6s remaining:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    2.6s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    2.7s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    2.7s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    2.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0178s.) Setting batch_size=22.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S1 ] [2 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n",
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S2 ] [3 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0166s.) Setting batch_size=24.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0177s.) Setting batch_size=22.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S3 ] [4 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n",
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S4 ] [5 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0160s.) Setting batch_size=24.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0158s.) Setting batch_size=24.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S5 ] [6 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n",
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S6 ] [7 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0160s.) Setting batch_size=24.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S7 ] [8 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n",
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S8 ] [9 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0153s.) Setting batch_size=26.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0154s.) Setting batch_size=26.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Deconvoluting dataset GSE134809 and sample [name: S9 ] [10 out of 10]\n",
      "Fitting 1 folds for each of 9 candidates, totalling 9 fits\n",
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0159s.) Setting batch_size=24.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# for nu-SVR\n",
    "result_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "param_df = pd.DataFrame(columns =['dataset.name', 'sample.id'])\n",
    "for dataset_name, c_type, scale, expression_path in list(zip(dataset_names, c_types, scales, expression_paths)):\n",
    "    samples_df = load_expression_file(expression_path)\n",
    "    \n",
    "    samples_df = linearize(samples_df, scale)\n",
    "    \n",
    "    # TODO this is dirty code\n",
    "    #c_type = 'BRCA'\n",
    "    \n",
    "    if c_type == 'BRCA':\n",
    "        signature_dream = brca_gep.copy()\n",
    "    else:\n",
    "        signature_dream = crc_gep.copy()\n",
    "   \n",
    "    for sample in samples_df:\n",
    "        print('Deconvoluting dataset {:6} and sample [name: {:3}] [{} out of {}]'.format(dataset_name, sample, samples_df.columns.get_loc(sample) + 1, samples_df.shape[1]))\n",
    "        \n",
    "        bulk_sample_subset, signature_matrix_subset = intersect_genes(samples_df[[sample]], signature_dream)\n",
    "        grid = build_model(bulk_sample_subset, signature_matrix_subset)\n",
    "        estimator = grid.best_estimator_.named_steps['svr']\n",
    "        fractions = estimator.coef_[0]\n",
    "        \n",
    "        fractions = rm_neg(fractions)\n",
    "        fractions = rescale(fractions)\n",
    "        \n",
    "        out_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "        out_df['cell.type'] = signature_matrix_subset.columns\n",
    "        out_df['prediction'] = fractions\n",
    "        out_df['dataset.name'] = dataset_name\n",
    "        out_df['sample.id'] = sample\n",
    "        \n",
    "        # TODO remove once integrated GEPs\n",
    "        if c_type != 'BRCA':\n",
    "            #out_df = append_missing_celltype(out_df, dataset_name, sample, cell_type=\"neutrophils\")\n",
    "            #mono = out_df.loc[out_df['cell.type']=='macrophages', 'prediction'].values + out_df.loc[out_df['cell.type']=='myeloid.dendritic.cells', 'prediction'].values\n",
    "            #out_df = append_missing_celltype(out_df, dataset_name, sample, cell_type=\"monocytes\", prediction = mono[0])\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        if docker:\n",
    "            out_path = os.path.join(out_dir, out_name)\n",
    "            result_df = result_df.append(out_df, ignore_index=True)\n",
    "            result_df.to_csv(out_path, header=True, index=False)\n",
    "        else:\n",
    "            out_name = str(len(load_df)) + \"_\" + str(type(estimator).__name__)\n",
    "            \n",
    "            # todo loop over param dictionary, add keys as columns, values as values.\n",
    "            \n",
    "            p_dict = {'dataset.name' : dataset_name, 'sample.id' : sample}\n",
    "\n",
    "            for key, value in grid.best_params_.items():\n",
    "                if str(key) not in param_df:\n",
    "                    param_df[str(key)] = np.nan\n",
    "                p_dict[str(key)] = value\n",
    "            \n",
    "            param_df = param_df.append(p_dict, ignore_index = True)\n",
    "            param_df.to_csv(os.path.join(out_dir,'param_' + out_name + gold_standards_file + '.csv'), header=True, index=False)\n",
    "            \n",
    "            out_path = os.path.join(out_dir, out_name + '_predict_' + gold_standards_file + '.csv')\n",
    "            result_df = result_df.append(out_df, ignore_index=True)\n",
    "            result_df.to_csv(out_path, header=True, index=False)\n",
    "\n",
    "if not docker:\n",
    "    load_row = [out_name + '_predict_' + gold_standards_file + '.csv', gold_standards_path]\n",
    "    load_df.loc[len(load_df)] = load_row\n",
    "    load_df.to_csv(os.path.join(out_dir, out_load), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nu-SVR speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- consider normalising both signature matrix and bulk vector to zero mean and unit variance - helps speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(bulk_sample_subset, signature_matrix_subset):\n",
    "    \"\"\"Create and fit a regression model to bulk RNA (y) and a signature matrix (X)\n",
    "    \n",
    "    y = w * X | w = 'predicted cell type fractions'\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        bulk_sample_subset (pandas.DataFrame): bulk RNA\n",
    "        signatre_matrix_subset (pandas.DataFrame): GEP\n",
    "    \n",
    "    Returns:\n",
    "        Instance of the regression model\n",
    "    \n",
    "    \"\"\"\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scale', StandardScaler()),\n",
    "        ('svr', NuSVR())\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = {'svr__nu' : [0.25, 0.5, 0.75], 'svr__C' : [1e-2,1e-1,1], 'svr__kernel' : ['linear'], 'svr__verbose' : [True]}\n",
    "    grid = GridSearchCV(pipe, param_grid = parameters, cv = 5, scoring = 'neg_mean_squared_error', verbose=10, n_jobs=-1)\n",
    "    grid.fit(signature_matrix_subset, bulk_sample_subset.values.ravel())\n",
    "\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn.svm.LinearSVR\n",
    "def build_model(bulk_sample_subset, signature_matrix_subset):\n",
    "    \"\"\"Create and fit a regression model to bulk RNA (y) and a signature matrix (X)\n",
    "    \n",
    "    y = w * X | w = 'predicted cell type fractions'\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        bulk_sample_subset (pandas.DataFrame): bulk RNA\n",
    "        signatre_matrix_subset (pandas.DataFrame): GEP\n",
    "    \n",
    "    Returns:\n",
    "        Instance of the regression model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model = LinearSVR()\n",
    "    parameters = {'epsilon' : [0.25, 0.5, 0.75], 'C' : [1e-2,1e-1,1], 'verbose' : [True], 'loss' : ['squared_epsilon_insensitive']}\n",
    "    grid = GridSearchCV(model, param_grid = parameters, cv = 5, scoring = 'neg_mean_squared_error', verbose=10, n_jobs=-1)\n",
    "    grid.fit(signature_matrix_subset, bulk_sample_subset.values.ravel())\n",
    "\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for linear svr\n",
    "result_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "param_df = pd.DataFrame(columns =['dataset.name', 'sample.id', 'epsilon', 'C'])\n",
    "for dataset_name, expression_path in list(zip(dataset_names, expression_paths)):\n",
    "    samples_df = load_expression_file(expression_path)\n",
    "    for sample in samples_df:\n",
    "        bulk_sample_subset, signature_matrix_subset = intersect_genes(samples_df[[sample]], signature_dream)\n",
    "        grid = build_model(bulk_sample_subset, signature_matrix_subset)\n",
    "        fractions = grid.best_estimator_.coef_\n",
    "        \n",
    "        fractions = rm_neg(fractions)\n",
    "        fractions = rescale(fractions)\n",
    "        \n",
    "        out_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "        out_df['cell.type'] = signature_matrix_subset.columns\n",
    "        out_df['prediction'] = fractions\n",
    "        out_df['dataset.name'] = dataset_name\n",
    "        out_df['sample.id'] = sample\n",
    "        \n",
    "        p_epsilon =  grid.best_params_['epsilon']\n",
    "        p_C = grid.best_params_['C']\n",
    "        p_row = [dataset_name, sample, p_epsilon, p_C]\n",
    "        param_df.loc[len(param_df)] = p_row\n",
    "        param_df.to_csv('param.csv', header=True, index=False)\n",
    "        \n",
    "        result_df = result_df.append(out_df, ignore_index=True)\n",
    "        result_df.to_csv(out_path, header=True, index=False) # because gridsearch for SVR so slow - output each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nu-SVR but with 5-fold CV\n",
    "def intersect_genes(bulk_sample, signature_matrix):\n",
    "    \"\"\"Find intersecting subset of genes between sample and signature matrix\"\"\"\n",
    "    #TODO add warning if only few genes are common\n",
    "    \n",
    "    # drop rows with nan\n",
    "    bulk_sample_notna = bulk_sample[pd.notna(bulk_sample.iloc[:,0:1]).any(axis=1)]\n",
    "    \n",
    "    idx = bulk_sample_notna.index & signature_matrix.index\n",
    "    return bulk_sample_notna.loc[idx], signature_matrix.loc[idx]\n",
    "\n",
    "\n",
    "def build_model(bulk_sample_subset, signature_matrix_subset):\n",
    "    \"\"\"Create and fit a regression model to bulk RNA (y) and a signature matrix (X)\n",
    "    \n",
    "    y = w * X | w = 'predicted cell type fractions'\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        bulk_sample_subset (pandas.DataFrame): bulk RNA\n",
    "        signatre_matrix_subset (pandas.DataFrame): GEP\n",
    "    \n",
    "    Returns:\n",
    "        Instance of the regression model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scale', StandardScaler()),\n",
    "        ('svr', NuSVR())\n",
    "    ])\n",
    "    \n",
    "    parameters = {'svr__nu' : [0.25, 0.5, 0.75], 'svr__C' : [1e-2,1e-1,1], 'svr__kernel' : ['linear'], 'svr__verbose' : [True]}\n",
    "    grid = GridSearchCV(pipe, param_grid = parameters, cv = cv, scoring = 'neg_mean_squared_error', verbose=10, n_jobs=-1)\n",
    "    grid.fit(signature_matrix_subset, bulk_sample_subset.values.ravel())\n",
    "\n",
    "    return grid\n",
    "\n",
    "# following methods are temporary\n",
    "def rm_neg(coefs):\n",
    "    \"\"\"Removes negative coefficients from the weight vector\n",
    "    \n",
    "    Args:\n",
    "        coefs (numpy.array): coefficients from the regression model, representing cell fractions\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array with non-negative values\n",
    "    \n",
    "    \"\"\"\n",
    "    c = coefs.copy()\n",
    "    c[c < 0] = 0 \n",
    "    return c\n",
    "\n",
    "def rm_small(coefs):\n",
    "    \"\"\"Remove small weight vector coefficents based on an arbitrary cutoff\"\"\"\n",
    "    # improvement: use standard deviation\n",
    "    c = coefs.copy()\n",
    "    c[c < 0.00001] = 0\n",
    "    return c\n",
    "\n",
    "def rescale (coefs):\n",
    "    \"\"\"Rescale the weight vector so that the total sums up to 1\"\"\"\n",
    "    c = coefs.copy()\n",
    "    scale_factor = 1.0 / (np.sum(c))\n",
    "    c *= scale_factor\n",
    "    return c\n",
    "\n",
    "def append_missing_celltype(df, dataset_name, sample, cell_type):\n",
    "    \"\"\"Appends missing cell type to the output\n",
    "    \n",
    "    Useful until we are able to integrate signatures with a full set of cell types\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): result df to be appended to\n",
    "        dataset_name (string)\n",
    "        sample (string)\n",
    "        cell_type (string)\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame with appended celltype and its predicted proportion set to 0\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    res_df = df.copy()\n",
    "    append_df = { \"dataset.name\" : dataset_name, \n",
    "                 \"sample.id\" : sample, \n",
    "                 \"cell.type\" : cell_type, \n",
    "                 \"prediction\" : 0}\n",
    "    res_df = res_df.append(append_df, ignore_index=True)\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GridSearchCV\n",
    "\"\"\"Issue with Ridge: it penalises model complexity while minimising variance in weights assigned to highly correlated predictors.\n",
    "Thus, we have issues deconvoluting highly correlated cell types e.g. Naive B Cell vs Memory B Cell.\n",
    "\"\"\"\n",
    "result_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "for dataset_name, expression_path in list(zip(dataset_names, expression_paths)):\n",
    "    samples_df = load_expression_file(expression_path)\n",
    "    for sample in samples_df:\n",
    "        bulk_sample_subset, signature_matrix_subset = intersect_genes(samples_df[[sample]], signature_dream)\n",
    "        grid = build_model(bulk_sample_subset, signature_matrix_subset)\n",
    "        fractions = grid.best_estimator_.coef_[0]\n",
    "        \n",
    "        fractions = rm_neg(fractions)\n",
    "        #fractions = rm_small(fractions)\n",
    "        fractions = rescale(fractions)\n",
    "        out_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "        out_df['cell.type'] = signature_matrix_subset.columns\n",
    "        out_df['prediction'] = fractions\n",
    "        out_df['dataset.name'] = dataset_name\n",
    "        out_df['sample.id'] = sample\n",
    "        \n",
    "        #temp\n",
    "        out_df = append_missing_celltype(dataset_name, sample, cell_type=\"neutrophils\")\n",
    "        out_df = append_missing_celltype(dataset_name, sample, cell_type=\"monocytes\")\n",
    "        \n",
    "        result_df = result_df.append(out_df, ignore_index=True)\n",
    "        result_df.to_csv(out_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random forest\n",
    "result_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "for dataset_name, expression_path in list(zip(dataset_names, expression_paths)):\n",
    "    samples_df = load_expression_file(expression_path)\n",
    "    for sample in samples_df:\n",
    "        bulk_sample_subset, signature_matrix_subset = intersect_genes(samples_df[[sample]], signature_dream)\n",
    "        model = build_model(bulk_sample_subset, signature_matrix_subset)\n",
    "\n",
    "        fractions = model.feature_importances_\n",
    "        \n",
    "        fractions = rm_neg(fractions)\n",
    "        #fractions = rm_small(fractions)\n",
    "        #fractions = rescale(fractions)\n",
    "        out_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "        out_df['cell.type'] = signature_matrix_subset.columns\n",
    "        out_df['prediction'] = fractions\n",
    "        out_df['dataset.name'] = dataset_name\n",
    "        out_df['sample.id'] = sample\n",
    "        result_df = result_df.append(out_df, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear regression\n",
    "result_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "for dataset_name, expression_path in list(zip(dataset_names, expression_paths)):\n",
    "    samples_df = load_expression_file(expression_path)\n",
    "    for sample in samples_df:\n",
    "        bulk_sample_subset, signature_matrix_subset = intersect_genes(samples_df[[sample]], signature_dream)\n",
    "        model = build_model(bulk_sample_subset, signature_matrix_subset)\n",
    "\n",
    "        fractions = model.coef_[0]\n",
    "        #fractions = model.best_estimator_.coef_\n",
    "        \n",
    "        fractions = rm_neg(fractions)\n",
    "        #fractions = rm_small(fractions)\n",
    "        fractions = rescale(fractions)\n",
    "        out_df = pd.DataFrame( columns = ['dataset.name', 'sample.id', 'cell.type', 'prediction'])\n",
    "        out_df['cell.type'] = signature_matrix_subset.columns\n",
    "        out_df['prediction'] = fractions\n",
    "        out_df['dataset.name'] = dataset_name\n",
    "        out_df['sample.id'] = sample\n",
    "        result_df = result_df.append(out_df, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
